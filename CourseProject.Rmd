---
title: "Machine Learning Course Project"
author: "Divya Panchal"
date: "October 12, 2017"
output: html_document
---

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Import Data
```{r}
library(caret); library(rattle); library(rpart); library(rpart.plot)
library(randomForest); library(repmis)
training <- read.csv("pml-training.csv", na.strings = c("NA", ""))
testing <- read.csv("pml-testing.csv", na.strings = c("NA", ""))
```
## Clean the data
```{r}
# Use only complete columns thus remove any columns with NA
training <- training[, colSums(is.na(training)) == 0]
testing <- testing[, colSums(is.na(testing)) == 0]
# Also remove first 7 columns as they are not the feature data set
trainData <- training[, -c(1:7)]
testData <- testing[, -c(1:7)]
```

## Split the data
```{r}
# Set seed
set.seed(7826) 
#Partition data
inTrain <- createDataPartition(trainData$classe, p = 0.7, list = FALSE)
train <- trainData[inTrain, ]
valid <- trainData[-inTrain, ]
```

## Model with Random Forest and Cross Validation
```{r}
library(randomForest)
modelFit_RF <- randomForest(classe ~ ., data=train, method="rf")
print(modelFit_RF)

# Plot Random Forest Model
plot(modelFit_RF)
varImpPlot(modelFit_RF)

pred_RF <- predict(modelFit_RF,newdata=valid)
# logic value for whether or not the rf algorithm predicted correctly
valid$predRight <- pred_RF==valid$classe
# tabulate results
print(table(pred_RF, valid$classe))

# Confusion Matrix
cm_RF <- confusionMatrix(pred_RF,valid$classe)
print(cm_RF)

# Plot Confusion Matrix
plot(cm_RF$table, col = cm_RF$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(cm_RF$overall['Accuracy'], 4)))
```

## Predict Activity Quality:
Now apply the model to testing data to predict activity quality.

```{r}
# Now apply the RF model to testing data and evaluate
test_result_RF <- predict(modelFit_RF, testing, type = "class")
# test_result_RF <- predict(modelFit_RF, testing)
print(test_result_RF)
summary(test_result_RF)
plot(test_result_RF)
```

## Generate answer files for assignment submission:
```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(test_result_RF)
```

## Conclusion:
Original Data source "http://groupware.les.inf.puc-rio.br/har", states that "Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes."  When I applied the model I created to testing data, it predicted that only **`r summary(test_result_RF)[[1]]`** out of **`r length(test_result_RF)`** subjects falls under Class A.  In conclusion, my model predicts that roughly 2/3 of people performs exercise incorrectly.


